{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from python_graphql_client import GraphqlClient\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = GraphqlClient(endpoint=\"https://medium.com/_/graphql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"query TopicHandler($topicSlug: ID!, $feedPagingOptions: PagingOptions, $sidebarPagingOptions: PagingOptions) {\\n  topic(slug: $topicSlug) {\\n    canonicalSlug\\n    ...TopicScreen_topic\\n    __typename\\n  }\\n}\\n\\nfragment PostListingItemFeed_postPreview on PostPreview {\\n  post {\\n    ...PostListingItemPreview_post\\n    ...PostListingItemByline_post\\n    ...PostListingItemImage_post\\n    ...PostPresentationTracker_post\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment PostListingItemPreview_post on Post {\\n  id\\n  mediumUrl\\n  title\\n  previewContent {\\n    subtitle\\n    isFullContent\\n    __typename\\n  }\\n  isPublished\\n  creator {\\n    id\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment PostListingItemByline_post on Post {\\n  id\\n  creator {\\n    id\\n    username\\n    name\\n    __typename\\n  }\\n  isLocked\\n  readingTime\\n  ...BookmarkButton_post\\n  firstPublishedAt\\n  updatedAt\\n statusForCollection\\n  collection {\\n    id\\n    name\\n    ...collectionUrl_collection\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment BookmarkButton_post on Post {\\n  ...SusiClickable_post\\n  ...WithSetReadingList_post\\n  __typename\\n}\\n\\nfragment SusiClickable_post on Post {\\n  id\\n  mediumUrl\\n  ...SusiContainer_post\\n  __typename\\n}\\n\\nfragment SusiContainer_post on Post {\\n  id\\n  __typename\\n}\\n\\nfragment WithSetReadingList_post on Post {\\n  ...ReadingList_post\\n  __typename\\n}\\n\\nfragment ReadingList_post on Post {\\n  __typename\\n  id\\n  readingList\\n}\\n\\nfragment collectionUrl_collection on Collection {\\n  id\\n  domain\\n  slug\\n  __typename\\n}\\n\\nfragment PostListingItemImage_post on Post {\\n  id\\n  mediumUrl\\n  previewImage {\\n    id\\n    focusPercentX\\n    focusPercentY\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment PostPresentationTracker_post on Post {\\n  id\\n  visibility\\n  previewContent {\\n    isFullContent\\n    __typename\\n  }\\n  collection {\\n    id\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment TopicScreen_topic on Topic {\\n  id\\n  ...TopicMetadata_topic\\n  ...TopicLandingHeader_topic\\n  ...TopicFeaturedAndLatest_topic\\n  ...TopicLandingRelatedTopics_topic\\n  ...TopicLandingPopular_posts\\n  __typename\\n}\\n\\nfragment TopicMetadata_topic on Topic {\\n  name\\n  description\\n  slug\\n  image {\\n    id\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment TopicLandingHeader_topic on Topic {\\n  name\\n  description\\n  visibility\\n  ...TopicFollowButtonSignedIn_topic\\n  ...TopicFollowButtonSignedOut_topic\\n  __typename\\n}\\n\\nfragment TopicFollowButtonSignedIn_topic on Topic {\\n  __typename\\n  slug\\n  isFollowing\\n}\\n\\nfragment TopicFollowButtonSignedOut_topic on Topic {\\n  id\\n  slug\\n  ...SusiClickable_topic\\n  __typename\\n}\\n\\nfragment SusiClickable_topic on Topic {\\n  ...SusiContainer_topic\\n  __typename\\n}\\n\\nfragment SusiContainer_topic on Topic {\\n  ...SignInOptions_topic\\n  ...SignUpOptions_topic\\n  __typename\\n}\\n\\nfragment SignInOptions_topic on Topic {\\n  id\\n  name\\n  __typename\\n}\\n\\nfragment SignUpOptions_topic on Topic {\\n  id\\n  name\\n  __typename\\n}\\n\\nfragment TopicFeaturedAndLatest_topic on Topic {\\n  name\\n  slug\\n  featuredPosts {\\n    postPreviews {\\n      post {\\n        id\\n        ...TopicLandingFeaturedStory_post\\n        __typename\\n      }\\n      __typename\\n    }\\n    __typename\\n  }\\n  featuredTopicWriters(limit: 1) {\\n    ...FeaturedWriter_featuredTopicWriter\\n    __typename\\n  }\\n  latestPosts(paging: $feedPagingOptions) {\\n    postPreviews {\\n      post {\\n        id\\n        __typename\\n      }\\n      ...PostListingItemFeed_postPreview\\n      __typename\\n    }\\n    pagingInfo {\\n      next {\\n        limit\\n        to\\n        __typename\\n      }\\n      __typename\\n    }\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment TopicLandingFeaturedStory_post on Post {\\n  ...FeaturedPostPreview_post\\n  ...PostListingItemPreview_post\\n  ...PostListingItemBylineWithAvatar_post\\n  ...PostListingItemImage_post\\n  ...PostPresentationTracker_post\\n  __typename\\n}\\n\\nfragment FeaturedPostPreview_post on Post {\\n  id\\n  title\\n  mediumUrl\\n  previewContent {\\n    subtitle\\n    isFullContent\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment PostListingItemBylineWithAvatar_post on Post {\\n  creator {\\n    username\\n    name\\n    id\\n    imageId\\n    mediumMemberAt\\n    __typename\\n  }\\n  isLocked\\n  readingTime\\n  updatedAt\\n  statusForCollection\\n  collection {\\n    id\\n    name\\n    ...collectionUrl_collection\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment FeaturedWriter_featuredTopicWriter on FeaturedTopicWriter {\\n  user {\\n    id\\n    username\\n    name\\n    bio\\n    ...UserAvatar_user\\n    ...UserFollowButton_user\\n    __typename\\n  }\\n  posts {\\n    postPreviews {\\n      ...PostListingItemFeaturedWriter_postPreview\\n      __typename\\n    }\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment UserAvatar_user on User {\\n  username\\n  id\\n  name\\n  imageId\\n  mediumMemberAt\\n  __typename\\n}\\n\\nfragment UserFollowButton_user on User {\\n  ...UserFollowButtonSignedIn_user\\n  ...UserFollowButtonSignedOut_user\\n  __typename\\n}\\n\\nfragment UserFollowButtonSignedIn_user on User {\\n  id\\n  isFollowing\\n  __typename\\n}\\n\\nfragment UserFollowButtonSignedOut_user on User {\\n  id\\n  ...SusiClickable_user\\n  __typename\\n}\\n\\nfragment SusiClickable_user on User {\\n  ...SusiContainer_user\\n  __typename\\n}\\n\\nfragment SusiContainer_user on User {\\n  ...SignInOptions_user\\n  ...SignUpOptions_user\\n  __typename\\n}\\n\\nfragment SignInOptions_user on User {\\n  id\\n  name\\n  __typename\\n}\\n\\nfragment SignUpOptions_user on User {\\n  id\\n  name\\n  __typename\\n}\\n\\nfragment PostListingItemFeaturedWriter_postPreview on PostPreview {\\n  postId\\n  post {\\n    readingTime\\n    id\\n    mediumUrl\\n    title\\n    ...PostListingItemImage_post\\n    ...PostPresentationTracker_post\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment TopicLandingRelatedTopics_topic on Topic {\\n  relatedTopics {\\n    topic {\\n      name\\n      slug\\n      __typename\\n    }\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment TopicLandingPopular_posts on Topic {\\n  name\\n  popularPosts(paging: $sidebarPagingOptions) {\\n    postPreviews {\\n      post {\\n        ...PostListingItemSidebar_post\\n        __typename\\n      }\\n      __typename\\n    }\\n    __typename\\n  }\\n  __typename\\n}\\n\\nfragment PostListingItemSidebar_post on Post {\\n  id\\n  mediumUrl\\n  title\\n  readingTime\\n  ...PostListingItemImage_post\\n  ...PostPresentationTracker_post\\n  __typename\\n}\\n\"\n",
    "\n",
    "variables = {\"topicSlug\": \"popular\", \"feedPagingOptions\": {\"limit\": 25, \"to\": f\"{int(time.time()*1000)}\"}, \"sidebarPagingOptions\": {\"limit\": 5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "data = client.execute(query=query, variables=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590519164019 - The Future of Movie Theaters Might Look a Lot Like an Apple Store - https://onezero.medium.com/the-future-of-movie-theaters-might-look-a-lot-like-an-apple-store-185c941d02a5\n",
      "1589979986031 - Inside the Flour Company Supplying America’s Sudden Baking Obsession - https://marker.medium.com/inside-the-flour-company-supplying-americas-sudden-baking-obsession-623034583579\n",
      "1590514172208 - Our Economy Was Just Blasted Years Into the Future - https://marker.medium.com/our-economy-was-just-blasted-years-into-the-future-a591fbba2298\n",
      "1590471061357 - The Unique Frustration of Being Black and in Charge - https://level.medium.com/the-unique-frustration-of-being-black-and-in-charge-c1026bb5a9b3\n",
      "1590384662134 - How Your Brain Prevents You From Getting Sick - https://elemental.medium.com/how-your-brain-prevents-you-from-getting-sick-201d8263344\n",
      "1590384661084 - A Decision-Making Framework for Re-Entering the World - https://forge.medium.com/a-decision-making-framework-for-re-entering-the-world-f3e2c6af883a\n",
      "1590190860889 - Brazil’s Covid-19 Crisis Shows What Will Happen if the U.S. Relaxes All Social Distancing Standards - https://coronavirus.medium.com/brazils-covid-19-crisis-shows-what-will-happen-if-the-us-relaxes-all-social-distancing-standards-b91380a367a\n",
      "1590169210045 - Social Distancing Is Not Enough - https://medium.com/the-atlantic/social-distancing-is-not-enough-5c56e9301304\n",
      "1590507346217 - My Journey Toward Radical Body Positivity - https://humanparts.medium.com/my-journey-toward-radical-body-positivity-3412796df8ff\n",
      "1590172851087 - How to Write Anything - https://forge.medium.com/how-to-write-anything-ad2a62c122f6\n"
     ]
    }
   ],
   "source": [
    "for post in data[\"data\"][\"topic\"][\"featuredPosts\"][\"postPreviews\"]:\n",
    "    print(f\"{post['post']['updatedAt']} - {post['post']['title']} - {post['post']['mediumUrl']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(data, posts):\n",
    "    for post in data[\"data\"][\"topic\"][\"latestPosts\"][\"postPreviews\"]:\n",
    "        title = \"\"\n",
    "        author = \"\"\n",
    "        author_username = \"\"\n",
    "        url = \"\"\n",
    "        domain = \"\"\n",
    "\n",
    "        if 'title' in post['post']:\n",
    "            title = post['post']['title']\n",
    "        if 'creator' in  post['post']:\n",
    "            creator = post['post']['creator'] or []\n",
    "            if 'username' in creator:\n",
    "                author = post['post']['creator']['name']\n",
    "                author_username = post['post']['creator']['username']\n",
    "        if 'mediumUrl' in post['post']:\n",
    "            url = post['post']['mediumUrl']\n",
    "        if 'collection' in post['post']:\n",
    "            collection = post['post']['collection'] or []\n",
    "            if 'domain' in collection:\n",
    "                domain = post['post']['collection']['domain']\n",
    "        post = {\n",
    "            \"title\": title, \n",
    "            \"author\": author,\n",
    "            \"author_username\": author_username,\n",
    "            \"url\": url,\n",
    "            \"domain\": domain\n",
    "        }\n",
    "        if post not in posts:\n",
    "            posts.append(post)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "nb_h_per_day = 6\n",
    "for i in range(365 * 24 // nb_h_per_day):\n",
    "    variables = {\"topicSlug\": \"popular\", \"feedPagingOptions\": {\"limit\": 25, \"to\": f\"{int(time.time() * 1000) - 3600000 * nb_h_per_day * i}\"}, \"sidebarPagingOptions\": {\"limit\": 1}}\n",
    "    data = client.execute(query=query, variables=variables)\n",
    "    posts = get_posts(data, posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', \"w\") as f:\n",
    "    json.dump(posts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_username</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Future of Movie Theaters Might Look a Lot ...</td>\n",
       "      <td>Eric Ravenscraft</td>\n",
       "      <td>lordravenscraft_50708</td>\n",
       "      <td>https://onezero.medium.com/the-future-of-movie...</td>\n",
       "      <td>onezero.medium.com</td>\n",
       "      <td>[amc, movies, amazon, netflix, apple]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inside the Flour Company Supplying America’s S...</td>\n",
       "      <td>David H. Freedman</td>\n",
       "      <td>dhfreedman2</td>\n",
       "      <td>https://marker.medium.com/inside-the-flour-com...</td>\n",
       "      <td>marker.medium.com</td>\n",
       "      <td>[king-arthur-flour, baking, business, consumer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our Economy Was Just Blasted Years Into the Fu...</td>\n",
       "      <td>Steve LeVine</td>\n",
       "      <td>stevlevine</td>\n",
       "      <td>https://marker.medium.com/our-economy-was-just...</td>\n",
       "      <td>marker.medium.com</td>\n",
       "      <td>[economy, business, society, future, automation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joe Biden’s Latest “Gaffe” Has Left Me Almost ...</td>\n",
       "      <td>Lauren Martinchek</td>\n",
       "      <td>xLauren_Mx</td>\n",
       "      <td>https://medium.com/discourse/joe-bidens-latest...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75 Things White People Can Do for Racial Justice</td>\n",
       "      <td>Corinne Shutack</td>\n",
       "      <td>cshutack</td>\n",
       "      <td>https://medium.com/equality-includes-you/what-...</td>\n",
       "      <td>None</td>\n",
       "      <td>[racism?source=post_page-----f2d18b0e0234-----...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title             author  \\\n",
       "0  The Future of Movie Theaters Might Look a Lot ...   Eric Ravenscraft   \n",
       "1  Inside the Flour Company Supplying America’s S...  David H. Freedman   \n",
       "2  Our Economy Was Just Blasted Years Into the Fu...       Steve LeVine   \n",
       "3  Joe Biden’s Latest “Gaffe” Has Left Me Almost ...  Lauren Martinchek   \n",
       "4   75 Things White People Can Do for Racial Justice    Corinne Shutack   \n",
       "\n",
       "         author_username                                                url  \\\n",
       "0  lordravenscraft_50708  https://onezero.medium.com/the-future-of-movie...   \n",
       "1            dhfreedman2  https://marker.medium.com/inside-the-flour-com...   \n",
       "2             stevlevine  https://marker.medium.com/our-economy-was-just...   \n",
       "3             xLauren_Mx  https://medium.com/discourse/joe-bidens-latest...   \n",
       "4               cshutack  https://medium.com/equality-includes-you/what-...   \n",
       "\n",
       "               domain                                                tag  \n",
       "0  onezero.medium.com              [amc, movies, amazon, netflix, apple]  \n",
       "1   marker.medium.com    [king-arthur-flour, baking, business, consumer]  \n",
       "2   marker.medium.com   [economy, business, society, future, automation]  \n",
       "3                None                                                 []  \n",
       "4                None  [racism?source=post_page-----f2d18b0e0234-----...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_num(s):\n",
    "    if isinstance(s, int):\n",
    "        return s\n",
    "    elif 'K' in s:\n",
    "        return int(1000 * float(s.replace('K','')))\n",
    "    else:\n",
    "        return int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in posts:\n",
    "    post['tag'] = []\n",
    "    post['claps'] = -1\n",
    "    try:\n",
    "        r = requests.get(post['url'])\n",
    "    except:\n",
    "        print(post['url'])\n",
    "        continue\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # tags\n",
    "    hrefs = soup.findAll('a', href=True)\n",
    "    for href in hrefs:\n",
    "        res = re.search(\"^/tagged/(.*)\", href['href'])\n",
    "        if res:\n",
    "            post['tag'].append(res.group(1))\n",
    "    # claps\n",
    "    buttons = soup.findAll('button')\n",
    "    for i, button in enumerate(buttons):\n",
    "        res = re.search(\"(.*) claps\", button.text)\n",
    "        if res:\n",
    "            post['claps'] = convert_to_num(res.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', \"w\") as f:\n",
    "    json.dump(posts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', \"r\") as f:\n",
    "    posts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_username</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>tag</th>\n",
       "      <th>claps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Future of Movie Theaters Might Look a Lot ...</td>\n",
       "      <td>Eric Ravenscraft</td>\n",
       "      <td>lordravenscraft_50708</td>\n",
       "      <td>https://onezero.medium.com/the-future-of-movie...</td>\n",
       "      <td>onezero.medium.com</td>\n",
       "      <td>[amc, movies, amazon, netflix, apple]</td>\n",
       "      <td>1.5K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inside the Flour Company Supplying America’s S...</td>\n",
       "      <td>David H. Freedman</td>\n",
       "      <td>dhfreedman2</td>\n",
       "      <td>https://marker.medium.com/inside-the-flour-com...</td>\n",
       "      <td>marker.medium.com</td>\n",
       "      <td>[king-arthur-flour, baking, business, consumer]</td>\n",
       "      <td>4.4K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our Economy Was Just Blasted Years Into the Fu...</td>\n",
       "      <td>Steve LeVine</td>\n",
       "      <td>stevlevine</td>\n",
       "      <td>https://marker.medium.com/our-economy-was-just...</td>\n",
       "      <td>marker.medium.com</td>\n",
       "      <td>[economy, business, society, future, automation]</td>\n",
       "      <td>3.6K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joe Biden’s Latest “Gaffe” Has Left Me Almost ...</td>\n",
       "      <td>Lauren Martinchek</td>\n",
       "      <td>xLauren_Mx</td>\n",
       "      <td>https://medium.com/discourse/joe-bidens-latest...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>5.9K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75 Things White People Can Do for Racial Justice</td>\n",
       "      <td>Corinne Shutack</td>\n",
       "      <td>cshutack</td>\n",
       "      <td>https://medium.com/equality-includes-you/what-...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>13.8K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title             author  \\\n",
       "0  The Future of Movie Theaters Might Look a Lot ...   Eric Ravenscraft   \n",
       "1  Inside the Flour Company Supplying America’s S...  David H. Freedman   \n",
       "2  Our Economy Was Just Blasted Years Into the Fu...       Steve LeVine   \n",
       "3  Joe Biden’s Latest “Gaffe” Has Left Me Almost ...  Lauren Martinchek   \n",
       "4   75 Things White People Can Do for Racial Justice    Corinne Shutack   \n",
       "\n",
       "         author_username                                                url  \\\n",
       "0  lordravenscraft_50708  https://onezero.medium.com/the-future-of-movie...   \n",
       "1            dhfreedman2  https://marker.medium.com/inside-the-flour-com...   \n",
       "2             stevlevine  https://marker.medium.com/our-economy-was-just...   \n",
       "3             xLauren_Mx  https://medium.com/discourse/joe-bidens-latest...   \n",
       "4               cshutack  https://medium.com/equality-includes-you/what-...   \n",
       "\n",
       "               domain                                               tag  claps  \n",
       "0  onezero.medium.com             [amc, movies, amazon, netflix, apple]   1.5K  \n",
       "1   marker.medium.com   [king-arthur-flour, baking, business, consumer]   4.4K  \n",
       "2   marker.medium.com  [economy, business, society, future, automation]   3.6K  \n",
       "3                None                                                []   5.9K  \n",
       "4                None                                                []  13.8K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "wtags = []\n",
    "for r,n in zip(df.tag, df.numclaps):\n",
    "    tags.extend(r)\n",
    "    wtags.extend(r * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = np.unique(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019 - 906\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tags)} - {len(unique_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health', 107),\n",
       " ('culture', 73),\n",
       " ('coronavirus', 59),\n",
       " ('politics', 53),\n",
       " ('digital-life', 51),\n",
       " ('life', 48),\n",
       " ('industry', 47),\n",
       " ('body', 46),\n",
       " ('technology', 37),\n",
       " ('science', 36),\n",
       " ('tech', 36),\n",
       " ('relationships', 34),\n",
       " ('self-improvement', 33),\n",
       " ('mental-health', 32),\n",
       " ('business', 31),\n",
       " ('self', 31),\n",
       " ('psychology', 30),\n",
       " ('food', 29),\n",
       " ('brain', 28),\n",
       " ('data-science', 28)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life-lessons', 789000),\n",
       " ('life', 784320),\n",
       " ('productivity', 777720),\n",
       " ('health', 737546),\n",
       " ('self-improvement', 658100),\n",
       " ('coronavirus', 645071),\n",
       " ('psychology', 644520),\n",
       " ('politics', 593697),\n",
       " ('mental-health', 538080),\n",
       " ('self', 527500),\n",
       " ('culture', 451832),\n",
       " ('inspiration', 438400),\n",
       " ('relationships', 367020),\n",
       " ('succeed', 320600),\n",
       " ('rethink', 267200),\n",
       " ('body', 266630),\n",
       " ('live', 247100),\n",
       " ('digital-life', 237861),\n",
       " ('economy', 230200),\n",
       " ('industry', 223428)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = Counter(wtags)\n",
    "wc.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numclaps'] = df.claps.apply(convert_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_username</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>tag</th>\n",
       "      <th>claps</th>\n",
       "      <th>numclaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Problem Isn’t Zoom Fatigue — It’s Mourning...</td>\n",
       "      <td>Evan Selinger</td>\n",
       "      <td>evanselinger</td>\n",
       "      <td>https://onezero.medium.com/the-problem-isnt-zo...</td>\n",
       "      <td>onezero.medium.com</td>\n",
       "      <td>[video-conferencing, digital-life, culture, zo...</td>\n",
       "      <td>1.2K</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>The Debate Over Screens and Health is More Con...</td>\n",
       "      <td>Markham Heid</td>\n",
       "      <td>mheidj</td>\n",
       "      <td>https://elemental.medium.com/kids-are-staring-...</td>\n",
       "      <td>elemental.medium.com</td>\n",
       "      <td>[health, science, brain, parenting, technology]</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>CEO of Surveillance Firm Banjo Once Helped KKK...</td>\n",
       "      <td>Matt Stroud</td>\n",
       "      <td>stroudjournalism</td>\n",
       "      <td>https://onezero.medium.com/ceo-of-surveillance...</td>\n",
       "      <td>onezero.medium.com</td>\n",
       "      <td>[surveillance, privacy, technology, racism, si...</td>\n",
       "      <td>2.9K</td>\n",
       "      <td>2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Zoom Is a Nightmare. So Why Is Everyone Still ...</td>\n",
       "      <td>Simon Pitt</td>\n",
       "      <td>simon_pitt</td>\n",
       "      <td>https://onezero.medium.com/zoom-is-a-nightmare...</td>\n",
       "      <td>onezero.medium.com</td>\n",
       "      <td>[zoom, video-conferencing, digital-life, techn...</td>\n",
       "      <td>23K</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>We Live in the Blue Ruin</td>\n",
       "      <td>Felicia C. Sullivan</td>\n",
       "      <td>felsull</td>\n",
       "      <td>https://humanparts.medium.com/blue-ruin-ab4b6d...</td>\n",
       "      <td>humanparts.medium.com</td>\n",
       "      <td>[fiction, dystopia, internet, technology, cult...</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title               author  \\\n",
       "25  The Problem Isn’t Zoom Fatigue — It’s Mourning...        Evan Selinger   \n",
       "61  The Debate Over Screens and Health is More Con...         Markham Heid   \n",
       "69  CEO of Surveillance Firm Banjo Once Helped KKK...          Matt Stroud   \n",
       "93  Zoom Is a Nightmare. So Why Is Everyone Still ...           Simon Pitt   \n",
       "97                           We Live in the Blue Ruin  Felicia C. Sullivan   \n",
       "\n",
       "     author_username                                                url  \\\n",
       "25      evanselinger  https://onezero.medium.com/the-problem-isnt-zo...   \n",
       "61            mheidj  https://elemental.medium.com/kids-are-staring-...   \n",
       "69  stroudjournalism  https://onezero.medium.com/ceo-of-surveillance...   \n",
       "93        simon_pitt  https://onezero.medium.com/zoom-is-a-nightmare...   \n",
       "97           felsull  https://humanparts.medium.com/blue-ruin-ab4b6d...   \n",
       "\n",
       "                   domain                                                tag  \\\n",
       "25     onezero.medium.com  [video-conferencing, digital-life, culture, zo...   \n",
       "61   elemental.medium.com    [health, science, brain, parenting, technology]   \n",
       "69     onezero.medium.com  [surveillance, privacy, technology, racism, si...   \n",
       "93     onezero.medium.com  [zoom, video-conferencing, digital-life, techn...   \n",
       "97  humanparts.medium.com  [fiction, dystopia, internet, technology, cult...   \n",
       "\n",
       "   claps  numclaps  \n",
       "25  1.2K      1200  \n",
       "61   508       508  \n",
       "69  2.9K      2900  \n",
       "93   23K     23000  \n",
       "97  1.3K      1300  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tech = df[df.tag.apply(lambda x: \"technology\" in x)]\n",
    "df_tech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_tags = []\n",
    "for r,n in zip(df_tech.tag, df_tech.numclaps):\n",
    "    tech_tags.extend(r * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('technology', 148447),\n",
       " ('digital-life', 43100),\n",
       " ('startup', 25900),\n",
       " ('video-conferencing', 24200),\n",
       " ('zoom', 24200),\n",
       " ('data-science', 23500),\n",
       " ('remote-working', 23000),\n",
       " ('artificial-intelligence', 22106),\n",
       " ('industry', 21900),\n",
       " ('programming', 16282)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tech = Counter(tech_tags)\n",
    "c_tech.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_tags = \"technology\", \"startup\", \"data-science\", \"artificial-intelligence\", \"programming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = \"29\"\n",
    "month = \"05\"\n",
    "year = \"2020\"\n",
    "tag = \"technology\"\n",
    "url = f\"https://medium.com/tag/{tag}/archive/{year}/{month}/{day}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div', class_=\"postArticle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/collindching/Maximizing-Medium-Claps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')  # Last I checked this was necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-20bc143a77b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchiveScraper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scraper_class = 'ArchiveScraper' \n",
    "    \n",
    "    def get_authors(self, soup):\n",
    "        try:\n",
    "            authors = []\n",
    "            \n",
    "            article_cards = soup.find_all('div',{'class': 'postArticle'})\n",
    "            for card in article_cards:\n",
    "                authors.append(card.select('a.ds-link')[0].text)\n",
    "                \n",
    "            return authors\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get authors.\")\n",
    "    \n",
    "    \n",
    "    def get_publications(self, soup):\n",
    "        try:\n",
    "            publications = []\n",
    "            \n",
    "            article_cards = soup.find_all('div',{'class': 'postArticle'})\n",
    "            for card in article_cards:        \n",
    "                published_by_soup = card.select('a.ds-link')\n",
    "    \n",
    "                if len(published_by_soup) == 1:\n",
    "                    publications.append('None')\n",
    "                    continue\n",
    "\n",
    "                publications.append(published_by_soup[1].text)\n",
    "\n",
    "            return publications\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get publications.\")\n",
    "    \n",
    "    \n",
    "    def get_titles(self, soup):\n",
    "        try:\n",
    "            titles = []\n",
    "            \n",
    "            article_cards = soup.find_all('div',{'class': 'postArticle'})\n",
    "            for card in article_cards:\n",
    "                title_soup = card.find('h3')\n",
    "\n",
    "                if title_soup is None:\n",
    "                    title = card.find('p').text\n",
    "                    titles.append(title)\n",
    "                else:\n",
    "                    title = card.find('h3').text\n",
    "                    titles.append(title)\n",
    "            \n",
    "            titles = [x.replace('\\xa0',' ').replace('\\u200a—\\u200a','--').replace('--', ' — ') for x in titles]\n",
    "            return titles\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get titles.\")\n",
    "    \n",
    "    \n",
    "    def get_read_times(self, soup):\n",
    "        try:\n",
    "            read_times = []\n",
    "            \n",
    "            read_time_soups = soup.find_all('span', {'class': 'readingTime'})\n",
    "            read_times = [int(x['title'].replace(\" min read\",\"\")) for x in read_time_soups]\n",
    "            \n",
    "            return read_times\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get read times.\")    \n",
    "            \n",
    "            \n",
    "    def count_responses(self, soup):\n",
    "        try:\n",
    "            responses = []\n",
    "\n",
    "            article_cards = soup.find_all('div', {'class': 'postArticle'})\n",
    "            for card in article_cards:\n",
    "                responses_text = card.select('div.buttonSet.u-floatRight')[0].text\n",
    "                if responses_text == '':\n",
    "                    responses.append(0)\n",
    "                else:\n",
    "                    responses.append(int(responses_text.replace(' responses','').replace(' response','')))\n",
    "        \n",
    "            return responses\n",
    "            \n",
    "        except:\n",
    "            print(\"Couldn't get responses.\")\n",
    "    \n",
    "    \n",
    "    def count_claps(self, soup):\n",
    "        try:\n",
    "            claps = []\n",
    "\n",
    "            article_cards = soup.find_all('div', {'class': 'postArticle'})\n",
    "            for card in article_cards:\n",
    "                clap_soup = card.find('button',{'data-action':'show-recommends'})\n",
    "\n",
    "                # no claps\n",
    "                if clap_soup is None:\n",
    "                    claps.append(0)\n",
    "                # >1k claps\n",
    "                elif 'K' in clap_soup.text:\n",
    "                    claps.append(int(float(clap_soup.text.replace('K',''))*1000))\n",
    "                # normal case\n",
    "                else:\n",
    "                    claps.append(int(clap_soup.text))\n",
    "                    \n",
    "            return claps\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get claps.\")\n",
    "            \n",
    "            \n",
    "    def get_dates(self, soup):\n",
    "        dates = []\n",
    "        article_cards = soup.find_all('div',{'class': 'postArticle'})\n",
    "        for i, card in enumerate(article_cards):\n",
    "            try:\n",
    "                dates.append(card.find('span',{'class':'middotDivider'}).previous_element)\n",
    "            \n",
    "            except:\n",
    "                print(\"Couldn't get this date.\")\n",
    "                dates.append('skip')\n",
    "                continue\n",
    "                \n",
    "        return dates\n",
    "            \n",
    "    def get_article_links(self, soup):\n",
    "        try:\n",
    "            links = []\n",
    "\n",
    "            article_cards = soup.find_all('div',{'class': 'postArticle'})\n",
    "            for card in article_cards:\n",
    "                links.append(card.find('div', {'class': 'postArticle-content'}).parent['href'])\n",
    "\n",
    "            return links\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get article links.\")\n",
    "            \n",
    "    \n",
    "    def scrape(self, soup):\n",
    "        archive_data = {\n",
    "            'authors': self.get_authors(soup),\n",
    "            'publications': self.get_publications(soup),\n",
    "            'titles': self.get_titles(soup),\n",
    "            'read_times': self.get_read_times(soup),\n",
    "            'n_responses': self.count_responses(soup),\n",
    "            'n_claps': self.count_claps(soup),\n",
    "            'dates': self.get_dates(soup),\n",
    "            'article_links': self.get_article_links(soup)\n",
    "        }\n",
    "        \n",
    "        return archive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleScraper():\n",
    "    \"\"\"\n",
    "    Scrapes all data for an article and returns it in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scraper_class = 'ArticleScraper' \n",
    "    \n",
    "    def get_title(self, soup):\n",
    "        try:\n",
    "            title = soup.find('h1').text\n",
    "            return title\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get title from article.\")\n",
    "        \n",
    "    def get_subtitle(self, soup):\n",
    "        try:\n",
    "            subtitle_soup = soup.find('h1').parent.parent.next_sibling.find('h2')\n",
    "            subtitle = clean_string(subtitle_soup.text)\n",
    "            return subtitle\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't (or didn't) get subtitle from article.\") \n",
    "            return 'None'\n",
    "            \n",
    "    def get_tags(self, soup):\n",
    "        try:\n",
    "            tags = []\n",
    "            tags = [x.text for x in soup.find_all(\"a\", href=re.compile(\".*tag.*\"))]\n",
    "            if len(tags) == 0:\n",
    "                return 'None'\n",
    "            else:\n",
    "                return tags\n",
    "            \n",
    "        except:\n",
    "            print(\"Couldn't get article tags.\")\n",
    "    \n",
    "    def get_author(self, soup):\n",
    "        try:\n",
    "            author = soup.find('div',style=re.compile(r'flex:.*')).find('a').text\n",
    "            return author\n",
    "        except:\n",
    "            print(\"Couldn't get author from article.\")\n",
    "            \n",
    "    def get_h1_headers(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            h1_header_soups = article_soup.find_all('h1')\n",
    "            \n",
    "            if len(h1_header_soups) == 1:\n",
    "                return 'None'\n",
    "            else:\n",
    "                h1_headers = [clean_string(x.text) for x in h1_header_soups[1:]]\n",
    "                return h1_headers\n",
    "        except:\n",
    "            print(\"Couldn't get h1 headers.\")\n",
    "            \n",
    "    \n",
    "    def get_h2_headers(self, soup):\n",
    "        try:\n",
    "            h2_headers = []\n",
    "            \n",
    "            article_soup = soup.find('article')\n",
    "            h2_header_soups = article_soup.find_all('h2')\n",
    "            \n",
    "            for header_soup in h2_header_soups:\n",
    "                if header_soup.text == \"Dive in. We'll learn what you like along the way.\":\n",
    "                    continue\n",
    "                else:\n",
    "                    h2_headers.append(clean_string(header_soup.text))\n",
    "            \n",
    "            if len(h2_headers) == 0:\n",
    "                return 'None'\n",
    "            else:\n",
    "                return h2_headers\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't get h2 headers.\")\n",
    "            \n",
    "    def get_paragraphs(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            paragraphs = [clean_string(x.text) for x in article_soup.find_all('p')]\n",
    "            return paragraphs\n",
    "        except:\n",
    "            print(\"Couldn't scrape paragraphs.\")\n",
    "            \n",
    "    def get_blockquotes(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            blockquotes = [x.text for x in article_soup.find_all('blockquote')]\n",
    "            return blockquotes\n",
    "        except:\n",
    "            print(\"Couldn't (or didn't) find blockquotes.\")\n",
    "            return 'None'\n",
    "            \n",
    "    def get_bolded(self,soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "\n",
    "            bolded = [x.text.strip() for x in article_soup.find_all('strong')]\n",
    "            bolded = [clean_string(x) for x in bolded if filter_string(x)]\n",
    "            bolded = [x for x in bolded if x]\n",
    "          \n",
    "            if len(bolded) == 0:\n",
    "                return 'None'\n",
    "            else:\n",
    "                return bolded\n",
    "        except:\n",
    "            print(\"Couldn't get bolded text.\")\n",
    "    \n",
    "    def get_italics(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            italics = [x.text.strip() for x in article_soup.find_all('em')]\n",
    "            italics = [clean_string(x) for x in italics if filter_string(x)]\n",
    "            \n",
    "            if len(italics) == 0:\n",
    "                return 'None'\n",
    "            else:\n",
    "                return italics\n",
    "        except:\n",
    "            print(\"Couldnt get italicized text.\")\n",
    "            \n",
    "    def count_bullet_lists(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            return len(article_soup.find_all('ul'))\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't count bullet lists.\")\n",
    "            \n",
    "    def count_numbered_lists(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            return len(article_soup.find_all('ol'))\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't count numbered lists.\")\n",
    "            \n",
    "    def count_figures(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            figures = article_soup.find_all('figure')\n",
    "            return len(figures)\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't count images.\")  \n",
    "            \n",
    "    def count_gists(self, soup):\n",
    "        try:\n",
    "            gists = []\n",
    "            article_soup = soup.find('article')\n",
    "            for fig in article_soup.find_all('figure'):\n",
    "                gist_soup = fig.find('iframe', title=re.compile('.*\\.py'))\n",
    "                if gist_soup == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    gists.append(gist_soup)\n",
    "                    \n",
    "            return len(gists)\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't count gists.\")\n",
    "            \n",
    "    def count_code_chunks(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            code_chunk_soups = article_soup.find_all('pre')\n",
    "            return len(code_chunk_soups)\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't count code chunks.\")\n",
    "            \n",
    "    def count_vids(self, soup):\n",
    "        try:\n",
    "            yt_vids = []\n",
    "            article_soup = soup.find('article')\n",
    "            for figure in article_soup.find_all('figure'):\n",
    "                yt_soup = figure.find('iframe', src=re.compile('.*youtube.*'))\n",
    "                if yt_soup == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    yt_vids.append(yt_soup)\n",
    "                    \n",
    "            return len(yt_vids)\n",
    "                    \n",
    "        except:\n",
    "            print(\"Couldn't get YouTube videos.\")        \n",
    "            \n",
    "    def count_links(self, soup):\n",
    "        try:\n",
    "            article_soup = soup.find('article')\n",
    "            link_soups = article_soup.find_all('a', {'target': '_blank'})\n",
    "            return len(link_soups)\n",
    "        \n",
    "        except:\n",
    "            print(\"Couldn't count links.\")      \n",
    "            \n",
    "    def scrape(self, soup):\n",
    "        article_data = {\n",
    "            \"title\": self.get_title(soup),\n",
    "            \"subtitle\": self.get_subtitle(soup),\n",
    "            \"tags\": self.get_tags(soup),\n",
    "            \"author\": self.get_author(soup),\n",
    "            \"h1_headers\": self.get_h1_headers(soup),\n",
    "            \"h2_headers\": self.get_h2_headers(soup),\n",
    "            \"paragraphs\": self.get_paragraphs(soup),\n",
    "            \"blockquotes\": self.get_blockquotes(soup),\n",
    "            \"bold_text\": self.get_bolded(soup),\n",
    "            \"italic_text\": self.get_italics(soup),\n",
    "            \"n_figures\": self.count_figures(soup),\n",
    "            \"n_bullet_lists\": self.count_bullet_lists(soup),\n",
    "            \"n_numbered_lists\": self.count_numbered_lists(soup),\n",
    "            \"n_gists\": self.count_gists(soup),\n",
    "            \"n_code_chunks\": self.count_code_chunks(soup),\n",
    "            \"n_vids\": self.count_vids(soup),\n",
    "            \"n_links\": self.count_links(soup),\n",
    "        }\n",
    "        \n",
    "        # if subtitle exists, remove it from h2_headers list\n",
    "        subtitle = article_data['subtitle']\n",
    "        if subtitle != 'None':\n",
    "            article_data['h2_headers'].remove(subtitle)\n",
    "            \n",
    "            if len(article_data['h2_headers']) == 0:\n",
    "                article_data['h2_headers'] = 'None'\n",
    "                \n",
    "        return(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
